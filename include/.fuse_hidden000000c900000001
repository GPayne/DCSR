#ifndef SPARSE_UPDATE_DEVICE
#define SPARSE_UPDATE_DEVICE

#define QUEUE_SIZE		512
#define WARP_SIZE 		32
#define LOG_WARP_SIZE	5

namespace device
{

__constant__ int chunk_sizes[8];

template <typename INDEX_TYPE, typename VALUE_TYPE>
__launch_bounds__(BLOCK_THREADS_MAX,1)
__global__ void 
OuterProduct(	const VALUE_TYPE *a,
				const VALUE_TYPE *b,
				const INDEX_TYPE num_rows,
				const INDEX_TYPE num_cols,
				const INDEX_TYPE num_cols_per_row,
				const INDEX_TYPE pitch,
				INDEX_TYPE *column_indices,
				VALUE_TYPE *values)
{
	const int tID = blockDim.x * blockIdx.x + threadIdx.x;
	const int grid_size = blockDim.x * gridDim.x;
	
	const INDEX_TYPE invalid_index = cusp::ell_matrix<int, INDEX_TYPE, cusp::device_memory>::invalid_index;

	__shared__ int entries_b[BLOCK_THREAD_SIZE];
	__shared__ int num_entries_a, num_entries_b;

	entries_b[threadIdx.x] = -1;
	__syncthreads();

	if(threadIdx.x == 0)		//first thread of every block
	{
		num_entries_a = 0;
		num_entries_b = 0;

		for(int i=0; i < num_rows; ++i)
		{
			if(a[i] != 0)
				num_entries_a++;
		}

		for(int i=0; i < num_cols; ++i)
		{
			if(b[i] != 0)
			{
				entries_b[num_entries_b] = i;
				num_entries_b++;
			}
		}
	}
	__syncthreads();

	for(int row=tID; row<num_rows; row+=grid_size)
	{
		int offset = row;
		if(a[row])
		{
			for(int n=0; n < num_entries_b; ++n, offset+=pitch)
			{
				column_indices[offset] = entries_b[n];
				values[offset] = 1;
			}
		}

		while(offset < num_cols_per_row*pitch)
		{
			column_indices[offset] = invalid_index;
			offset += pitch;
		}
	}
}

//size of a must be num_rows + 1
//size of b must be num_cols + 1
//last entry of each array is used for storing entry count
//only add unique entries
template <typename INDEX_TYPE, typename VALUE_TYPE>
__launch_bounds__(BLOCK_THREADS_MAX,1)
__global__ void 
OuterProductAdd_ELL_B(	const VALUE_TYPE *a,
						const VALUE_TYPE *b,
						const INDEX_TYPE *index_count,
						const INDEX_TYPE num_rows,
						const INDEX_TYPE num_cols,
						const INDEX_TYPE num_cols_per_row,
						const INDEX_TYPE pitch,
						INDEX_TYPE *column_indices,
						VALUE_TYPE *values)
{
	const int tID = threadIdx.x & (WARP_SIZE-1); 									//thread ID
	const int wID = (blockDim.x * blockIdx.x + threadIdx.x) / WARP_SIZE;			//warp ID
	const int grid_size = (blockDim.x * gridDim.x) / WARP_SIZE;
	const INDEX_TYPE invalid_index = cusp::ell_matrix<INDEX_TYPE, VALUE_TYPE, cusp::device_memory>::invalid_index;

	INDEX_TYPE num_entries_a = index_count[0];
	INDEX_TYPE num_entries_b = index_count[1];
	__shared__ INDEX_TYPE row_index[BLOCK_THREAD_SIZE/WARP_SIZE];

	for(INDEX_TYPE j=wID; j < num_entries_a; j+=grid_size)
	{
		INDEX_TYPE row = a[j];
		row_index[wID] = column_indices[row];
		for(INDEX_TYPE k=tID; k < num_entries_b; k+=WARP_SIZE)
		{
			VALUE_TYPE b_col = b[k];
			INDEX_TYPE offset = row;
			for(INDEX_TYPE n=1; n < num_cols_per_row; ++n, offset+=pitch)
			{
				INDEX_TYPE col = column_indices[offset];
				if(col == b_col)
				{
					break;
				}
				else if(col == invalid_index)
				{
					column_indices[row*(atomicAdd(&row_index[wID],1)+1)] = b_col;
					//values[offset] = 1;
					break;
				}
			}
		}
		if(tID == 0)
			column_indices[row] = row_index[wID];
	}
}

//size of a must be num_rows + 1
//size of b must be num_cols + 1
//last entry of each array is used for storing entry count
//only add unique entries
template <typename INDEX_TYPE, typename VALUE_TYPE>
__launch_bounds__(BLOCK_THREADS_MAX,1)
__global__ void 
OuterProductAdd_HYB_B(	const VALUE_TYPE *a,
						const VALUE_TYPE *b,
						const INDEX_TYPE *index_count,
						const INDEX_TYPE num_rows,
						const INDEX_TYPE num_cols,
						const INDEX_TYPE num_cols_per_row,
						const INDEX_TYPE pitch,
						INDEX_TYPE *row_sizes,
						INDEX_TYPE *column_indices,
						INDEX_TYPE *coo_row_indices,
						INDEX_TYPE *coo_column_indices)
{
	//const int tID = threadIdx.x & (WARP_SIZE-1); 									//thread ID
	//const int wID = (blockDim.x * blockIdx.x + threadIdx.x) / WARP_SIZE;			//warp ID
	//const int grid_size = (blockDim.x * gridDim.x) / WARP_SIZE;
	const int tID = blockDim.x * blockIdx.x + threadIdx.x;
	const int grid_size = blockDim.x * gridDim.x;
	const INDEX_TYPE invalid_index = cusp::ell_matrix<INDEX_TYPE, VALUE_TYPE, cusp::device_memory>::invalid_index;

	INDEX_TYPE num_entries_a = index_count[0];
	INDEX_TYPE num_entries_b = index_count[1];

	//for(INDEX_TYPE j=wID; j < num_entries_a; j+=grid_size)
	for(INDEX_TYPE j=tID; j < num_entries_a; j+=grid_size)
	{
		INDEX_TYPE row = a[j];
		//for(INDEX_TYPE k=tID; k < num_entries_b; k+=WARP_SIZE)
		for(INDEX_TYPE k=0; k<num_entries_b; ++k)
		{
			VALUE_TYPE b_col = b[k];
			INDEX_TYPE offset = row;
			bool overflow = false;
			for(INDEX_TYPE n=1; n < num_cols_per_row; ++n, offset+=pitch)
			{
				INDEX_TYPE col = column_indices[offset];
				if(col == b_col)
				{
					if(n == num_cols_per_row-1)
						overflow = true;
					break;
				}
				else if(col == invalid_index)
				{
					INDEX_TYPE index = atomicAdd(&row_sizes[row], 1);
					column_indices[row + pitch*index] = b_col;
					break;
				}
			}

			//coordinate overflow
			if(overflow)
			{
				bool valid = true;
				for(int i=1; i < coo_column_indices[0]; ++i)
				{
					if(coo_column_indices[i] == b_col && coo_row_indices[i] == row)
					{
						valid = false;
						break;
					}
				}

				if(valid)
				{
					int index = atomicAdd(&coo_column_indices[0], 1)+1;
					coo_row_indices[index] = row;
					coo_column_indices[index] = b_col;
				}
			}
		}
	}
}

//size of a must be num_rows + 1
//size of b must be num_cols + 1
//last entry of each array is used for storing entry count
//only add unique entries
template <typename INDEX_TYPE, typename VALUE_TYPE>
__launch_bounds__(BLOCK_THREADS_MAX,1)
__global__ void 
OuterProductAdd_Queue(	const VALUE_TYPE *a,
						const VALUE_TYPE *b,
						const INDEX_TYPE *index_count,
						INDEX_TYPE *queue)
{
	//const int tID = threadIdx.x & (WARP_SIZE-1); 									//thread ID
	//const int wID = (blockDim.x * blockIdx.x + threadIdx.x) / WARP_SIZE;			//warp ID
	//const int grid_size = (blockDim.x * gridDim.x) / WARP_SIZE;
	const int tID = blockDim.x * blockIdx.x + threadIdx.x;
	const int grid_size = blockDim.x * gridDim.x;
	//const INDEX_TYPE invalid_index = -1;

	INDEX_TYPE num_entries_a = index_count[0];
	INDEX_TYPE num_entries_b = index_count[1];
	//INDEX_TYPE qstart = queue[0];
	INDEX_TYPE qstop = queue[1];

	if(tID == 0)
	{
		queue[qstop] = num_entries_a;
		queue[qstop+1] = num_entries_b;
	}
	__syncthreads();

	for(INDEX_TYPE j=tID; j < num_entries_a; j+=grid_size)
	{
		queue[qstop+2 + j] = a[j];
	}
	
	for(INDEX_TYPE k=tID; k<num_entries_b; k+=grid_size)
	{
		queue[qstop+2 + num_entries_a + k] = b[k];
	}
	__syncthreads();

	if(tID == 0)
	{
		queue[1] += (num_entries_a + num_entries_b + 2);
	}
	__syncthreads();
}

template <typename INDEX_TYPE, typename VALUE_TYPE, unsigned int BINS, unsigned int VECTORS_PER_BLOCK, unsigned int THREADS_PER_VECTOR>
__launch_bounds__(256,1)			//<-- Check this....  seriously WTF...
__global__ void 
add_matrix_dcsr(	const INDEX_TYPE num_rows,
					const INDEX_TYPE pitchA,
					const INDEX_TYPE pitchB,
					INDEX_TYPE *A_MD,
					INDEX_TYPE *Aj,
					VALUE_TYPE *Ax,
					INDEX_TYPE *A_row_offsets,
					INDEX_TYPE *A_row_sizes,
					const INDEX_TYPE *B_MD,
					const INDEX_TYPE *Bj,
					const VALUE_TYPE *Bx,
					const INDEX_TYPE *B_row_offsets,
					const INDEX_TYPE *B_row_sizes)
{
	const INDEX_TYPE THREADS_PER_BLOCK = VECTORS_PER_BLOCK * THREADS_PER_VECTOR;

	const INDEX_TYPE thread_id   = THREADS_PER_BLOCK * blockIdx.x + threadIdx.x;    // global thread index
	const INDEX_TYPE thread_lane = threadIdx.x & (THREADS_PER_VECTOR - 1);          // thread index within the vector
	const INDEX_TYPE vector_id   = thread_id   /  THREADS_PER_VECTOR;               // global vector index
	//const INDEX_TYPE vector_lane = threadIdx.x /  THREADS_PER_VECTOR;            	// vector index within the block
	const INDEX_TYPE num_vectors = VECTORS_PER_BLOCK * gridDim.x;                   // total number of active vectors

	for(INDEX_TYPE row=vector_id; row < num_rows; row+=num_vectors)
	{
		INDEX_TYPE Aoffset = 0;
		INDEX_TYPE rlA = A_row_sizes[row];
		INDEX_TYPE AR_idx = 0, A_idx, A_row_start, A_row_end;
		A_row_start = A_row_offsets[row*2];
		A_row_end = A_row_offsets[row*2 + 1];
		INDEX_TYPE free_mem = 0;

		while(AR_idx < rlA)
		{
			AR_idx += A_row_end - A_row_start;
			if(AR_idx < rlA)
			{
				Aoffset++;
				A_row_start = A_row_offsets[Aoffset*pitchA + row*2];
				A_row_end = A_row_offsets[Aoffset*pitchA + row*2 + 1];
			}
		}
		if(AR_idx > rlA)
		{
			A_idx = A_row_start + (A_row_end - A_row_start) - (AR_idx - rlA);
			AR_idx = rlA;
			free_mem = A_row_end - A_idx;
		}
		else
			A_idx = A_row_end;

		INDEX_TYPE Boffset = 0;
		INDEX_TYPE rlB = B_row_sizes[row];
		INDEX_TYPE BR_idx = thread_lane, B_idx, B_row_start, B_row_end;
		B_row_start = B_row_offsets[row*2];
		B_row_end = B_row_offsets[row*2 + 1];
		B_idx = B_row_start + thread_lane;
		A_idx += thread_lane;

		//allocate new space for chunk
		if(thread_lane == 0 && free_mem < rlB && rlB > 0)
		{
			INDEX_TYPE new_size = rlB - free_mem;
			INDEX_TYPE new_addr = atomicAdd(&A_MD[0], new_size);	//increase global memory pointer
			
			A_row_offsets[(Aoffset+1)*pitchA + row*2] = new_addr;
			A_row_offsets[(Aoffset+1)*pitchA + row*2 + 1] = new_addr + new_size;
		}

		while(BR_idx < rlB)
		{
			for(B_idx; B_idx < B_row_end && BR_idx < rlB; B_idx+=THREADS_PER_VECTOR, BR_idx+=THREADS_PER_VECTOR)
			{
				if(A_idx >= A_row_end)
				{
					INDEX_TYPE pos = A_idx - A_row_end;
					Aoffset++;
					A_row_start = A_row_offsets[Aoffset*pitchA + row*2];
					A_row_end = A_row_offsets[Aoffset*pitchA + row*2 + 1];
					A_idx = A_row_start + pos;
				}

				Aj[A_idx] = Bj[B_idx];
				Ax[A_idx] = Bx[B_idx];

				A_idx += THREADS_PER_VECTOR;
			}

			if(B_idx >= B_row_end && BR_idx < rlB)
			{
				INDEX_TYPE pos = B_idx - B_row_end;
				Boffset++;
				B_row_start = B_row_offsets[Boffset*pitchB + row*2];
				B_row_end = B_row_offsets[Boffset*pitchB + row*2 + 1];
				B_idx = B_row_start + pos;
			}
		}

		if(thread_lane == 0)
			A_row_sizes[row] += rlB;
	}
}

template <typename INDEX_TYPE, typename VALUE_TYPE, unsigned int BINS, unsigned int VECTORS_PER_BLOCK, unsigned int THREADS_PER_VECTOR>
__launch_bounds__(256,1)			//<-- Check this....  seriously WTF...
__global__ void 
add_matrix_ell_dell(	const INDEX_TYPE num_rows,
						const INDEX_TYPE pitchA,
						const INDEX_TYPE pitchB,
						const INDEX_TYPE ell_widthA,
						const INDEX_TYPE ell_widthB,
						INDEX_TYPE *A_ell_column_indices,
						VALUE_TYPE *A_ell_values,
						INDEX_TYPE *A_MD,
						INDEX_TYPE *Aj,
						VALUE_TYPE *Ax,
						INDEX_TYPE *A_row_offsets,
						INDEX_TYPE *A_row_sizes,
						const INDEX_TYPE *Bj,
						const VALUE_TYPE *Bx,
						const INDEX_TYPE *B_row_offsets,
						const INDEX_TYPE *B_row_sizes)
{
	const INDEX_TYPE THREADS_PER_BLOCK = VECTORS_PER_BLOCK * THREADS_PER_VECTOR;

	const INDEX_TYPE thread_id   = THREADS_PER_BLOCK * blockIdx.x + threadIdx.x;    // global thread index
	const INDEX_TYPE thread_lane = threadIdx.x & (THREADS_PER_VECTOR - 1);          // thread index within the vector
	const INDEX_TYPE vector_id   = thread_id   /  THREADS_PER_VECTOR;               // global vector index
	const INDEX_TYPE vector_lane = threadIdx.x /  THREADS_PER_VECTOR;             	// vector index within the block
	const INDEX_TYPE num_vectors = VECTORS_PER_BLOCK * gridDim.x;                   // total number of active vectors

	__shared__ volatile unsigned char wcount[VECTORS_PER_BLOCK];
	__shared__ volatile INDEX_TYPE size[VECTORS_PER_BLOCK];
	__shared__ volatile INDEX_TYPE addr[VECTORS_PER_BLOCK];

	for(INDEX_TYPE row=vector_id; row < num_rows; row+=num_vectors)
	{
		INDEX_TYPE Boffset = 0;
		INDEX_TYPE rlB = B_row_sizes[row];
		INDEX_TYPE B_idx, B_row_start, B_row_end;

		for(B_idx=thread_lane; B_idx < ell_widthB && B_idx < rlB; B_idx+=THREADS_PER_VECTOR)
		{
			INDEX_TYPE col = Bj[row + B_idx*pitchB];
			
			unsigned char valid = 1;
			INDEX_TYPE Aoffset = 0;
			INDEX_TYPE rlA = A_row_sizes[row];
			INDEX_TYPE AR_idx = 0, A_idx, A_row_start, A_row_end;

			//Check ELL Entries
			for(AR_idx = thread_lane; AR_idx < rlA && AR_idx < ell_widthA && valid; AR_idx++)
			{
				if(A_ell_column_indices[row + AR_idx*pitchA] == col)
				{
					A_ell_column_indices[row + AR_idx*pitchA] += Bx[row + B_idx*pitchB];
					valid = 0;
					break;
				}
			}
			
			if(rlA < ell_widthA)
			{
				INDEX_TYPE pos = valid;
				#if(THREADS_PER_VECTOR == 32)
					warpScan32(pos, thread_lane);
				#elif(THREADS_PER_VECTOR == 16)
					warpScan16(pos, thread_lane);
				#endif
				if(thread_lane == THREADS_PER_VECTOR-1)
					wcount[vector_lane] = pos;

				if(wcount[vector_lane] > 0)
				{
					if(thread_lane == 0 && A_row_start + wcount[vector_lane] >= ell_widthA)
					{
						//allocate new space for chunk
						size[vector_lane] = max(ell_widthA * 2, 32);
						addr[vector_lane] = atomicAdd(&A_MD[0], size[vector_lane]);	//increase global memory pointer
					}

					if(AR_idx + pos < ell_widthA && valid)
					{
						A_ell_column_indices[row + (AR_idx + pos)*pitchA] = col;
						A_ell_values[row + (AR_idx + pos)*pitchA] = Bx[row + B_idx*pitchB];
						valid = 0;
					}
					else
					{
						A_idx = (AR_idx + pos) - ell_widthA;
						Aj[A_idx] = col;
						Ax[A_idx] = Bx[row + B_idx*pitchB];
						valid = 0;
					}

					if(thread_lane == 0)
						A_row_sizes[row] += wcount[vector_lane];
				}
			}

			//Check DCSR Entries
			A_idx = A_row_start = A_row_offsets[row*2];
			A_row_end = A_row_offsets[row*2 + 1];

			while(AR_idx < rlA && valid)
			{
				for(A_idx = A_row_start; A_idx < A_row_end && AR_idx < rlA; A_idx++, AR_idx++)
				{
					if(Aj[A_idx] == col)
					{
						Ax[A_idx] += Bx[row + B_idx*pitchB];
						valid = 0;
						break;
					}
				}

				if(A_idx >= A_row_end)
				{
					Aoffset++;
					A_idx = A_row_start = A_row_offsets[Aoffset*pitchA + row*2];
					A_row_end = A_row_offsets[Aoffset*pitchA + row*2+1];
				}
			}

			INDEX_TYPE pos = valid;
			#if(THREADS_PER_VECTOR == 32)
				warpScan32(pos, thread_lane);
			#elif(THREADS_PER_VECTOR == 16)
				warpScan16(pos, thread_lane);
			#endif
			if(thread_lane == THREADS_PER_VECTOR-1)
				wcount[vector_lane] = pos;

			if(wcount[vector_lane] > 0)
			{
				if(thread_lane == 0 && A_idx + wcount[vector_lane] >= A_row_end)
				{
					//allocate new space for chunk
					size[vector_lane] = max((A_row_offsets[(Aoffset-1)*pitchA + row*2 + 1] - A_row_offsets[(Aoffset-1)*pitchA + row*2]) * (Aoffset+1), 32);
					addr[vector_lane] = atomicAdd(&A_MD[0], size[vector_lane]);	//increase global memory pointer

					A_row_offsets[Aoffset*pitchA + row*2] = addr[vector_lane] + ((A_row_start + pos) - A_row_end);
					A_row_offsets[Aoffset*pitchA + row*2+1] = addr[vector_lane] + size[vector_lane];
				}

				if(valid)
				{
					//check for threads that cross bin boundary
					if(A_idx + pos >= A_row_end)
					{
						if(A_idx < A_row_end)
							Aoffset++;

						pos = (A_idx + pos) - A_row_end;
						A_idx = A_row_offsets[Aoffset*pitchA + row*2];

						Aj[A_idx + pos] = col;
						Ax[A_idx + pos] = Bx[row + B_idx*pitchB];
					}
					else
					{
						Aj[A_idx + pos] = col;
						Ax[A_idx + pos] = Bx[row + B_idx*pitchB];
					}
				}

				if(thread_lane == 0)
					A_row_sizes[row] += wcount[vector_lane];
			}
		}

		if(B_idx >= B_row_end)
		{
			Boffset++;
			B_idx = B_row_start = B_row_offsets[Boffset*pitchB + row*2];
			B_row_end = B_row_offsets[Boffset*pitchB + row*2+1];
		}
	}
}

//*****************************************************************************//
//update matrices with arrays of row, column and value indices
//*****************************************************************************//
template <typename INDEX_TYPE, typename VALUE_TYPE, unsigned int VECTORS_PER_BLOCK, unsigned int THREADS_PER_VECTOR>
__launch_bounds__(BLOCK_THREAD_SIZE,1)
__global__ void 
UpdateMatrix_dcsr(	const INDEX_TYPE num_rows,
					const INDEX_TYPE pitch,
					const INDEX_TYPE *src_rows,
					const INDEX_TYPE *src_cols,
					const VALUE_TYPE *src_vals,
					const INDEX_TYPE *offsets,
					INDEX_TYPE *A_MD,
					INDEX_TYPE *Aj,
					VALUE_TYPE *Ax,
					INDEX_TYPE *row_offsets,
					INDEX_TYPE *row_sizes)
{
	//const INDEX_TYPE tID = blockDim.x*blockIdx.x + threadIdx.x;
	//const INDEX_TYPE grid_size = blockDim.x * gridDim.x;				//grid size

	const INDEX_TYPE THREADS_PER_BLOCK = VECTORS_PER_BLOCK * THREADS_PER_VECTOR;

	const INDEX_TYPE thread_id   = THREADS_PER_BLOCK * blockIdx.x + threadIdx.x;    // global thread index
	const INDEX_TYPE thread_lane = threadIdx.x & (THREADS_PER_VECTOR - 1);          // thread index within the vector
	const INDEX_TYPE vector_id   = thread_id   /  THREADS_PER_VECTOR;               // global vector index
	//const INDEX_TYPE vector_lane = threadIdx.x /  THREADS_PER_VECTOR;             	// vector index within the block
	const INDEX_TYPE num_vectors = VECTORS_PER_BLOCK * gridDim.x;                   // total number of active vectors

	for(INDEX_TYPE row=vector_id; row < num_rows; row+=num_vectors)
	{
		INDEX_TYPE Aoffset = 0;
		INDEX_TYPE rlA = row_sizes[row];
		INDEX_TYPE AR_idx = 0, A_idx, A_row_start, A_row_end;
		A_row_start = row_offsets[row*2];
		A_row_end = row_offsets[row*2 + 1];
		INDEX_TYPE free_mem = 0;

		while(AR_idx < rlA)
		{
			AR_idx += A_row_end - A_row_start;
			if(AR_idx < rlA)
			{
				Aoffset++;
				A_row_start = row_offsets[Aoffset*pitch + row*2];
				A_row_end = row_offsets[Aoffset*pitch + row*2 + 1];
			}
		}
		if(AR_idx > rlA)
		{
			A_idx = A_row_start + (A_row_end - A_row_start) - (AR_idx - rlA);
			AR_idx = rlA;
			free_mem = A_row_end - A_idx;
		}
		else
			A_idx = A_row_end;

		INDEX_TYPE B_row_start = offsets[row];
		INDEX_TYPE B_row_end = offsets[row+1];
		INDEX_TYPE rlB = B_row_end - B_row_start;
		A_idx += thread_lane;

		//allocate new space for chunk
		if(thread_lane == 0 && free_mem < rlB && rlB > 0)
		{
			INDEX_TYPE new_size = rlB - free_mem;
			INDEX_TYPE new_addr = atomicAdd(&A_MD[0], new_size);	//increase global memory pointer
			
			row_offsets[(Aoffset+1)*pitch + row*2] = new_addr;
			row_offsets[(Aoffset+1)*pitch + row*2 + 1] = new_addr + new_size;
		}

		for(INDEX_TYPE B_idx = B_row_start + thread_lane; B_idx < B_row_end; B_idx+=THREADS_PER_VECTOR, A_idx+=THREADS_PER_VECTOR)
		{
			if(A_idx >= A_row_end)
			{
				INDEX_TYPE pos = A_idx - A_row_end;
				Aoffset++;
				A_row_start = row_offsets[Aoffset*pitch + row*2];
				A_row_end = row_offsets[Aoffset*pitch + row*2 + 1];
				A_idx = A_row_start + pos;
			}

			Aj[A_idx] = src_cols[B_idx];
			Ax[A_idx] = src_vals[B_idx];
		}
	}
}

template <typename INDEX_TYPE, typename VALUE_TYPE>
__launch_bounds__(BLOCK_THREAD_SIZE,1)
__global__ void 
UpdateMatrix_hyb_B(	const INDEX_TYPE num_rows,
					const INDEX_TYPE num_cols,
					const INDEX_TYPE num_cols_per_row,
					const INDEX_TYPE pitch,
					const INDEX_TYPE *src_rows,
					const INDEX_TYPE *src_cols,
					const INDEX_TYPE N,
					INDEX_TYPE *rs,
					INDEX_TYPE *column_indices,
					INDEX_TYPE *overflow_rows,
					INDEX_TYPE *overflow_cols)
{
	const int tID = blockDim.x * blockIdx.x + threadIdx.x;
	const int grid_size = blockDim.x * gridDim.x;

	for(INDEX_TYPE row=tID; row < num_rows; row+=grid_size)
	{
		for(INDEX_TYPE i=0; i < N; i++)
		{
			if(src_rows[i] == row)
			{
				INDEX_TYPE offset = row;
				INDEX_TYPE col = src_cols[i];
				INDEX_TYPE rl = rs[row];
				bool valid = true;

				// for(INDEX_TYPE j=0; j < rl && valid; j++)
				// {
				// 	if(column_indices[offset + j*pitch] == col)
				// 	{
				// 		valid = false;
				// 		break;
				// 	}
				// }

				if(rl < num_cols_per_row && valid)
				{
					column_indices[offset + rl*pitch] = col;
					rs[row] += 1;
					valid = false;
				}
				else if(valid) 	//overflow
				{
					bool ovf_valid = true;
					// for(INDEX_TYPE i=1; i <= rs[num_rows]; ++i)
					// {
					// 	if(overflow_cols[i] == col && overflow_rows[i] == row)
					// 	{
					// 		ovf_valid = false;
					// 		break;
					// 	}
					// }

					if(ovf_valid)
					{
						INDEX_TYPE index = atomicAdd(&rs[num_rows], 1);
						overflow_rows[index] = row;
						overflow_cols[index] = col;
					}
				}
			}
		}
	}
}

template <typename INDEX_TYPE, typename VALUE_TYPE>
__launch_bounds__(BLOCK_THREAD_SIZE,1)
__global__ void 
UpdateMatrix_hyb(	const INDEX_TYPE num_rows,
					const INDEX_TYPE num_cols,
					const INDEX_TYPE num_cols_per_row,
					const INDEX_TYPE pitch,
					const INDEX_TYPE *src_rows,
					const INDEX_TYPE *src_cols,
					const VALUE_TYPE *src_vals,
					const INDEX_TYPE *offsets,
					INDEX_TYPE *row_sizes,
					INDEX_TYPE *column_indices,
					VALUE_TYPE *vals,
					INDEX_TYPE *overflow_rows,
					INDEX_TYPE *overflow_cols,
					VALUE_TYPE *overflow_vals)
{
	const int tID = blockDim.x * blockIdx.x + threadIdx.x;
	const int grid_size = blockDim.x * gridDim.x;

	for(INDEX_TYPE row=tID; row < num_rows; row+=grid_size)
	{
		INDEX_TYPE row_start = offsets[row];
		INDEX_TYPE row_end = offsets[row+1];

		for(INDEX_TYPE j = row_start; j < row_end; j++)
		{
			INDEX_TYPE offset = row;
			INDEX_TYPE col = src_cols[j];
			VALUE_TYPE val = src_vals[j];
			INDEX_TYPE rl = row_sizes[row];
			INDEX_TYPE idx;
			bool valid = true;

			// for(idx = 0; idx < rl && idx < num_cols_per_row && valid; idx++)
			// {
			// 	if(column_indices[offset + idx*pitch] == col)
			// 	{
			// 		valid = false;
			// 		break;
			// 	}
			// }

			if(rl < num_cols_per_row && valid)
			{
				column_indices[offset + idx*pitch] = col;
				vals[offset + idx*pitch] = val;
				row_sizes[row] += 1;
				valid = false;
			}
			else if(valid) 	//overflow
			{
				// //get end index of sorted values
				// INDEX_TYPE send = row_sizes[num_rows+1];
				// INDEX_TYPE sstart = 0, mid, pos = -1;

				// //binary search of sorted section
				// while(sstart != send)
				// {
				// 	if(send - sstart <= 1)
				// 	{
				// 		if(overflow_rows[sstart] == row)
				// 			pos = sstart;
				// 		break;
				// 	}

				// 	mid = (send - sstart) / 2 + sstart;
				// 	if(overflow_rows[mid] > row)
				// 		send = mid;
				// 	else if(overflow_rows[mid] < row)
				// 		sstart = mid;
				// 	else if(overflow_rows[mid] == row)
				// 	{	
				// 		for(pos=mid; pos > 0; pos--)
				// 		{
				// 			if(overflow_rows[pos-1] != row)
				// 				break;
				// 		}
				// 		break;
				// 	}
				// }

				bool ovf_valid = true;
				// if(pos > -1)
				// {
				// 	send = row_sizes[num_rows+1];
				// 	for(INDEX_TYPE j=pos; j < send && overflow_rows[row] == row; ++j)
				// 	{
				// 		if(overflow_cols[j] == col)
				// 		{
				// 			ovf_valid = false;
				// 			break;
				// 		}
				// 	}
				// }

				// sstart = row_sizes[num_rows+1];
				// send = row_sizes[num_rows];
				// for(INDEX_TYPE j=sstart; j <= send && ovf_valid; ++j)
				// {
				// 	if(overflow_cols[j] == col && overflow_rows[j] == row)
				// 	{
				// 		ovf_valid = false;
				// 		break;
				// 	}
				// }

				if(ovf_valid)
				{
					INDEX_TYPE index = atomicAdd(&row_sizes[num_rows], 1);
					row_sizes[row] += 1;
					overflow_rows[index] = row;
					overflow_cols[index] = col;
					overflow_vals[index] = val;
				}
			}
		}
	}
}

template <typename INDEX_TYPE, typename VALUE_TYPE>
__launch_bounds__(BLOCK_THREAD_SIZE,1)
__global__ void 
UpdateMatrix_dell(	const INDEX_TYPE num_rows,
					const INDEX_TYPE pitch,
					const INDEX_TYPE num_cols_per_row,
					const INDEX_TYPE *src_rows,
					const INDEX_TYPE *src_cols,
					const VALUE_TYPE *src_vals,
					const INDEX_TYPE N,
					INDEX_TYPE *ell_column_indices,
					VALUE_TYPE *ell_values,
					INDEX_TYPE *Matrix_MD,
					INDEX_TYPE *Aj,
					VALUE_TYPE *Ax,
					INDEX_TYPE *row_offsets,
					INDEX_TYPE *row_sizes)
{
	const INDEX_TYPE tID = blockDim.x*blockIdx.x + threadIdx.x;
	const INDEX_TYPE grid_size = blockDim.x * gridDim.x;				//grid size

	for(INDEX_TYPE row=tID; row < num_rows; row+=grid_size)
	{
		//loop of N inputs looking for elements from the respective row
		for(INDEX_TYPE i=0; i < N; i++)
		{
			if(src_rows[i] == row)
			{
				INDEX_TYPE col = src_cols[i];
				VALUE_TYPE val = src_vals[i];
				INDEX_TYPE rl = row_sizes[row];

				bool valid = true;
				INDEX_TYPE offset = 0;
				INDEX_TYPE r_idx = 0, idx, row_start, row_end;
				idx = row_start = row_offsets[row*2];
				row_end = row_offsets[row*2 + 1];

				for(idx = 0; idx < rl && idx < num_cols_per_row && valid; idx++)
				{
					if(ell_column_indices[row + idx*pitch] == col)
					{
						valid = false;
						break;
					}
				}
				if(rl < num_cols_per_row && valid)
				{
					ell_column_indices[row + idx*pitch] = col;
					ell_values[row + idx*pitch] = val;
					row_sizes[row] += 1;
					valid = false;
				}

				while(r_idx < rl && valid)
				{
					for(idx = row_start; idx < row_end && r_idx < rl; idx++, r_idx++)
					{
						if(Aj[idx] == col)
						{
							valid = false;
							break;
						}
					}

					if(idx >= row_end)
					{
						offset++;
						idx = row_start = row_offsets[offset*pitch + row*2];
						row_end = row_offsets[offset*pitch + row*2+1];
					}
				}

				if(valid)
				{
					//if non allocated memory section then allocate new section of memory for this row
					if(row_start == row_end)
					{
						//allocate new space for chunk
						INDEX_TYPE new_size = max((row_offsets[(offset-1)*pitch + row*2 + 1] - row_offsets[(offset-1)*pitch + row*2]) * (offset+1), 32);
						INDEX_TYPE new_add = atomicAdd(&Matrix_MD[0], new_size);	//increase global memory pointer

						//allocate new row chunk...
						idx = row_offsets[offset*pitch + row*2] = new_add;
						row_offsets[offset*pitch + row*2+1] = new_add + new_size;
					}

					Aj[idx] = col;
					Ax[idx] = val;
					row_sizes[row] += 1;
				}
			}
		}
	}
}

//*******************************************************************************//
//Load matrix from coo matrix.  Assume no duplicate entries.
//*******************************************************************************//
template <typename INDEX_TYPE, typename VALUE_TYPE>
__launch_bounds__(BLOCK_THREAD_SIZE,1)
__global__ void 
LoadMatrix_dcsr_B_coo(	const INDEX_TYPE num_rows,
						const INDEX_TYPE chunk_size,
						const INDEX_TYPE pitch,
						const float alpha,
						const INDEX_TYPE *src_rows,
						const INDEX_TYPE *src_cols,
						const INDEX_TYPE N,
						INDEX_TYPE *Matrix_MD,
						INDEX_TYPE *ci,
						INDEX_TYPE *cl,
						INDEX_TYPE *ca,
						INDEX_TYPE *rs,
						INDEX_TYPE *cols)
{
	const int tID = blockDim.x * blockIdx.x + threadIdx.x;		//thread ID
	//const int lID = threadIdx.x;								//lane ID
	const int grid_size = blockDim.x * gridDim.x;				//grid size

	for(INDEX_TYPE row=tID; row < num_rows; row+=grid_size)
	{

	}
}

template <typename INDEX_TYPE, typename VALUE_TYPE>
__launch_bounds__(BLOCK_THREAD_SIZE,1)
__global__ void 
LoadMatrix_dcsr_coo(	const INDEX_TYPE num_rows,
						const INDEX_TYPE chunk_size,
						const INDEX_TYPE pitch,
						const float alpha,
						const INDEX_TYPE *src_rows,
						const INDEX_TYPE *src_cols,
						const VALUE_TYPE *src_vals,
						const INDEX_TYPE N,
						INDEX_TYPE *Matrix_MD,
						INDEX_TYPE *ci,
						INDEX_TYPE *cl,
						INDEX_TYPE *ca,
						INDEX_TYPE *rs,
						INDEX_TYPE *cols,
						VALUE_TYPE *vals)
{
	// const int tID = blockDim.x * blockIdx.x + threadIdx.x;		//thread ID
	// const int btID = threadIdx.x;								//block thread ID
	// const int grid_size = blockDim.x * gridDim.x;				//grid size

	// __shared__ volatile INDEX_TYPE rs_s[BLOCK_THREAD_SIZE];

	// for(INDEX_TYPE row=tID; row < num_rows; row+=grid_size)
	// {

	// }
}

template <typename INDEX_TYPE, typename VALUE_TYPE>
__launch_bounds__(BLOCK_THREAD_SIZE,1)
__global__ void 
LoadMatrix_hyb_B_coo(	const INDEX_TYPE num_rows,
						const INDEX_TYPE num_cols,
						const INDEX_TYPE num_cols_per_row,
						const INDEX_TYPE pitch,
						const INDEX_TYPE *src_rows,
						const INDEX_TYPE *src_cols,
						const INDEX_TYPE N,
						INDEX_TYPE *rs,
						INDEX_TYPE *column_indices,
						VALUE_TYPE *vals,
						INDEX_TYPE *overflow_rows,
						INDEX_TYPE *overflow_cols)
{
	const int tID = blockDim.x * blockIdx.x + threadIdx.x;
	const int grid_size = blockDim.x * gridDim.x;

	for(INDEX_TYPE row=tID; row < num_rows; row+=grid_size)
	{
		for(INDEX_TYPE i=0; i < N; i++)
		{
			if(src_rows[i] == row)
			{
				INDEX_TYPE offset = row;
				INDEX_TYPE col = src_cols[i];
				INDEX_TYPE rl = rs[row];

				if(rl < num_cols_per_row)
				{
					column_indices[offset + rl*pitch] = col;
					rs[row] += 1;
				}
				else //overflow
				{
					INDEX_TYPE index = atomicAdd(&rs[num_rows], 1);
					rs[row] += 1;
					overflow_rows[index] = row;
					overflow_cols[index] = col;
				}
			}
		}
	}
}

template <typename INDEX_TYPE, typename VALUE_TYPE, unsigned int VECTORS_PER_BLOCK, unsigned int THREADS_PER_VECTOR>
__launch_bounds__(BLOCK_THREAD_SIZE,1)
__global__ void 
LoadMatrix_hyb_coo(	const INDEX_TYPE num_rows,
					const INDEX_TYPE num_cols,
					const INDEX_TYPE num_cols_per_row,
					const INDEX_TYPE pitch,
					const INDEX_TYPE *row_offsets,
					const INDEX_TYPE N,
					INDEX_TYPE *rs,
					INDEX_TYPE *column_indices,
					VALUE_TYPE *vals,
					INDEX_TYPE *overflow_rows,
					INDEX_TYPE *overflow_cols,
					VALUE_TYPE *overflow_vals)
{
    const INDEX_TYPE THREADS_PER_BLOCK = VECTORS_PER_BLOCK * THREADS_PER_VECTOR;

    const INDEX_TYPE thread_id   = THREADS_PER_BLOCK * blockIdx.x + threadIdx.x;    // global thread index
    const INDEX_TYPE thread_lane = threadIdx.x & (THREADS_PER_VECTOR - 1);          // thread index within the vector
    const INDEX_TYPE vector_id   = thread_id   /  THREADS_PER_VECTOR;               // global vector index
    //const INDEX_TYPE vector_lane = threadIdx.x /  THREADS_PER_VECTOR;               // vector index within the block
    const INDEX_TYPE num_vectors = VECTORS_PER_BLOCK * gridDim.x;                   // total number of active vectors

	for(INDEX_TYPE row=vector_id; row < num_rows; row+=num_vectors)
	{
		INDEX_TYPE row_start = row_offsets[row];
		INDEX_TYPE row_end = row_offsets[row+1];
		INDEX_TYPE size = row_end - row_start;

		for(INDEX_TYPE i=row_start+thread_lane; i < (row_start + num_cols_per_row) && i < row_end; i+=THREADS_PER_VECTOR)
		{
			INDEX_TYPE offset = i - row_start;
			column_indices[row + offset*pitch] = overflow_cols[i];
			vals[row + offset*pitch] = overflow_vals[i];
			overflow_rows[i] = INT_MAX;
		}

		if(thread_lane == 0)
		{
			rs[row] = size;
			atomicAdd(&rs[num_rows], -min(size, num_cols_per_row));
		}
	}
}

template <typename INDEX_TYPE, typename VALUE_TYPE>
__launch_bounds__(BLOCK_THREAD_SIZE,1)
__global__ void 
LoadMatrix_csr_coo(	const INDEX_TYPE num_rows,
					const INDEX_TYPE *src_rows,
					const INDEX_TYPE *src_cols,
					const VALUE_TYPE *src_vals,
					const INDEX_TYPE N,
					INDEX_TYPE *T_i,
					INDEX_TYPE *A_j,
					VALUE_TYPE *A_x)
{
	const int tID = blockDim.x * blockIdx.x + threadIdx.x;
	const int grid_size = blockDim.x * gridDim.x;

	for(INDEX_TYPE i=tID; i < N; i+=grid_size)
	{
		T_i[i] = src_rows[i];
		A_j[i] = src_cols[i];
		A_x[i] = src_vals[i];
	}
}

template <typename INDEX_TYPE, typename VALUE_TYPE>
__launch_bounds__(BLOCK_THREAD_SIZE,1)
__global__ void 
LoadMatrix_csr_count_rows(	const INDEX_TYPE num_rows,
							const INDEX_TYPE *src_rows,
							const INDEX_TYPE N,
							INDEX_TYPE *row_offsets)
{
	const int tID = blockDim.x * blockIdx.x + threadIdx.x;
	const int grid_size = blockDim.x * gridDim.x;

	for(INDEX_TYPE row=tID; row < num_rows; row+=grid_size)
	{
		for(INDEX_TYPE i=0; i < N; i++)
		{
			if(src_rows[i] == row)
			{
				row_offsets[row] += 1;
			}
		}
	}
}

template <typename INDEX_TYPE, typename VALUE_TYPE>
__launch_bounds__(BLOCK_THREAD_SIZE,1)
__global__ void 
ConvertMatrix_DELL_CSR(	const INDEX_TYPE num_rows,
						const INDEX_TYPE chunk_size,
						const INDEX_TYPE pitch,
						const INDEX_TYPE *Matrix_MD,
						const INDEX_TYPE *ci,
						const INDEX_TYPE *cl,
						const INDEX_TYPE *ca,
						const INDEX_TYPE *rs,
						const INDEX_TYPE *src_column_indices,
						const VALUE_TYPE *src_values,
						INDEX_TYPE *row_offsets,
						INDEX_TYPE *dst_column_indices,
						VALUE_TYPE *dst_values)
{
	const int tID = blockDim.x * blockIdx.x + threadIdx.x;
	const int grid_size = blockDim.x * gridDim.x;

	for(INDEX_TYPE row=tID; row < num_rows; row += grid_size)
    {
        INDEX_TYPE rl = rs[row];
        INDEX_TYPE r_idx = 0;
        INDEX_TYPE cID = row / chunk_size;
        bool next_chunk = false;

        do
        {
            INDEX_TYPE next_cID = ci[cID];
            //INDEX_TYPE offset = A_ca[cID] + (row % chunk_size);
            INDEX_TYPE offset = ca[cID] + (row & (chunk_size-1))*pitch;
            INDEX_TYPE clength = cl[cID];

            INDEX_TYPE csr_row_start = row_offsets[row];
            //csr_row_end = row_offsets[row+1];

            for(INDEX_TYPE c_idx = 0; c_idx < clength && r_idx < rl; ++c_idx, ++r_idx)
            {
            	dst_column_indices[csr_row_start + r_idx] = src_column_indices[offset + c_idx];
                dst_values[csr_row_start + r_idx] = src_values[offset + c_idx];
            }

            if(next_cID > 0 && r_idx < rl)
            {
                next_chunk = true;
                cID = next_cID;
            }
            else
                next_chunk = false;

        } while(next_chunk);
    }
}

//*******************************************************************************//
//Sort DELL matrix rows
//*******************************************************************************//
template <typename INDEX_TYPE, typename VALUE_TYPE, unsigned int BINS, unsigned int VECTORS_PER_BLOCK, unsigned int THREADS_PER_VECTOR>
__launch_bounds__(BLOCK_THREAD_SIZE,1)
__global__ void
SetRowIndices(	const INDEX_TYPE num_rows,
				const INDEX_TYPE pitch,
				INDEX_TYPE *Ai,
				const INDEX_TYPE *A_row_offsets,
				const INDEX_TYPE *A_rs)
{
    __shared__ volatile INDEX_TYPE ptrs[VECTORS_PER_BLOCK][2];

    const INDEX_TYPE THREADS_PER_BLOCK = VECTORS_PER_BLOCK * THREADS_PER_VECTOR;

    const INDEX_TYPE thread_id   = THREADS_PER_BLOCK * blockIdx.x + threadIdx.x;    // global thread index
    const INDEX_TYPE thread_lane = threadIdx.x & (THREADS_PER_VECTOR - 1);          // thread index within the vector
    const INDEX_TYPE vector_id   = thread_id   /  THREADS_PER_VECTOR;               // global vector index
    const INDEX_TYPE vector_lane = threadIdx.x /  THREADS_PER_VECTOR;               // vector index within the block
    const INDEX_TYPE num_vectors = VECTORS_PER_BLOCK * gridDim.x;                   // total number of active vectors

    for(INDEX_TYPE row = vector_id; row < num_rows; row += num_vectors)
    {
    	INDEX_TYPE r_idx = 0;
        const INDEX_TYPE rl = A_rs[row];

    	#pragma unroll
        for(INDEX_TYPE offset = 0; offset < BINS; offset++)
        {
            // use two threads to fetch A_row_offsets[row] and A_row_offsets[row+1]
            // this is considerably faster than the straightforward version
            #if(THREADS_PER_VECTOR >= 2)
            if(thread_lane < 2)
            {
                ptrs[vector_lane][thread_lane] = A_row_offsets[offset*pitch + row*2 + thread_lane];
                //ptrs[vector_lane][1] = A_row_offsets[offset*pitch + row*2 + 1];
            }
            #else
            {
                ptrs[vector_lane][0] = A_row_offsets[offset*pitch + row*2];
                ptrs[vector_lane][1] = A_row_offsets[offset*pitch + row*2 + 1];
            }
            #endif

            const INDEX_TYPE row_start = ptrs[vector_lane][0];          	//same as: row_start = A_row_offsets[row];
            INDEX_TYPE row_end = ptrs[vector_lane][1];						//same as: row_end   = A_row_offsets[row+1];
            INDEX_TYPE row_startE = row_end;
            const INDEX_TYPE row_endE = row_end;

            if(row_start == row_end)
            	break;					//exit out of loop if range size is 0

            if(row_end - row_start > rl - r_idx)
            {
            	row_end = row_startE = row_start + rl - r_idx;
            }

            if(THREADS_PER_VECTOR == 32 && row_end - row_start > 32)
            {
                // ensure aligned memory access to Aj and Ax
                INDEX_TYPE jj = row_start - (row_start & (THREADS_PER_VECTOR - 1)) + thread_lane;

                // accumulate local sums
                if(jj >= row_start && jj < row_end)
            		Ai[jj] = row;

                // accumulate local sums
                for(jj += THREADS_PER_VECTOR; jj < row_end; jj += THREADS_PER_VECTOR)
                    Ai[jj] = row;
            }
            else
            {
                // accumulate local sums
                for(INDEX_TYPE jj = row_start + thread_lane; jj < row_end; jj += THREADS_PER_VECTOR)
                    Ai[jj] = row;
            }

            //assign extra row slots with a value of INT_MAX
            for(INDEX_TYPE jj = row_startE + thread_lane; jj < row_endE; jj += THREADS_PER_VECTOR)
            	Ai[jj] = INT_MAX;

            r_idx = min(rl, r_idx + (row_end - row_start));
        }
    }
}

template <typename INDEX_TYPE, typename VALUE_TYPE, unsigned int BINS, unsigned int VECTORS_PER_BLOCK, unsigned int THREADS_PER_VECTOR>
__launch_bounds__(BLOCK_THREAD_SIZE,1)
__global__ void
SetIndices(	const INDEX_TYPE num_rows,
			const INDEX_TYPE pitch,
			INDEX_TYPE *Ai,
			INDEX_TYPE *Aj,
			const INDEX_TYPE *A_row_offsets,
			const INDEX_TYPE *A_rs)
{
    __shared__ volatile INDEX_TYPE ptrs[VECTORS_PER_BLOCK][2];

    const INDEX_TYPE THREADS_PER_BLOCK = VECTORS_PER_BLOCK * THREADS_PER_VECTOR;

    const INDEX_TYPE thread_id   = THREADS_PER_BLOCK * blockIdx.x + threadIdx.x;    // global thread index
    const INDEX_TYPE thread_lane = threadIdx.x & (THREADS_PER_VECTOR - 1);          // thread index within the vector
    const INDEX_TYPE vector_id   = thread_id   /  THREADS_PER_VECTOR;               // global vector index
    const INDEX_TYPE vector_lane = threadIdx.x /  THREADS_PER_VECTOR;               // vector index within the block
    const INDEX_TYPE num_vectors = VECTORS_PER_BLOCK * gridDim.x;                   // total number of active vectors

    for(INDEX_TYPE row = vector_id; row < num_rows; row += num_vectors)
    {
    	INDEX_TYPE r_idx = 0;
        const INDEX_TYPE rl = A_rs[row];

    	#pragma unroll
        for(INDEX_TYPE offset = 0; offset < BINS; offset++)
        {
            // use two threads to fetch A_row_offsets[row] and A_row_offsets[row+1]
            // this is considerably faster than the straightforward version
            #if(THREADS_PER_VECTOR >= 2)
            if(thread_lane < 2)
            {
                ptrs[vector_lane][thread_lane] = A_row_offsets[offset*pitch + row*2 + thread_lane];
                //ptrs[vector_lane][1] = A_row_offsets[offset*pitch + row*2 + 1];
            }
            #else
            {
                ptrs[vector_lane][0] = A_row_offsets[offset*pitch + row*2];
                ptrs[vector_lane][1] = A_row_offsets[offset*pitch + row*2 + 1];
            }
            #endif

            const INDEX_TYPE row_start = ptrs[vector_lane][0];          	//same as: row_start = A_row_offsets[row];
            INDEX_TYPE row_end = ptrs[vector_lane][1];						//same as: row_end   = A_row_offsets[row+1];
            INDEX_TYPE row_startE = row_end;
            const INDEX_TYPE row_endE = row_end;

            if(row_start == row_end)
            	break;					//exit out of loop if range size is 0

            if(row_end - row_start > rl - r_idx)
            {
            	row_end = row_startE = row_start + rl - r_idx;
            }

            if(THREADS_PER_VECTOR == 32 && row_end - row_start > 32)
            {
                // ensure aligned memory access to Aj and Ax
                INDEX_TYPE jj = row_start - (row_start & (THREADS_PER_VECTOR - 1)) + thread_lane;

                // accumulate local sums
                if(jj >= row_start && jj < row_end)
            		Ai[jj] = row;

                // accumulate local sums
                for(jj += THREADS_PER_VECTOR; jj < row_end; jj += THREADS_PER_VECTOR)
                    Ai[jj] = row;
            }
            else
            {
                // accumulate local sums
                for(INDEX_TYPE jj = row_start + thread_lane; jj < row_end; jj += THREADS_PER_VECTOR)
                    Ai[jj] = row;
            }

            //assign extra row slots with a value of -1
            for(INDEX_TYPE jj = row_startE + thread_lane; jj < row_endE; jj += THREADS_PER_VECTOR)
            {
            	Ai[jj] = INT_MAX;
            	Aj[jj] = INT_MAX;
            }

            r_idx = min(rl, r_idx + (row_end - row_start));
        }
    }
}

template <typename INDEX_TYPE, typename VALUE_TYPE, unsigned int BINS>
__launch_bounds__(BLOCK_THREAD_SIZE,1)
__global__ void
SetOffsets(	const INDEX_TYPE num_rows,
			const INDEX_TYPE pitch,
			INDEX_TYPE *Matrix_MD,
			INDEX_TYPE *A_o,
			const INDEX_TYPE *temp_offsets)
{
	const int tID = blockDim.x * blockIdx.x + threadIdx.x;			// global thread index
	const int grid_size = blockDim.x * gridDim.x;
    
    for(INDEX_TYPE row = tID; row < num_rows; row += grid_size)
    {
        A_o[row*2] = temp_offsets[row];
        A_o[row*2 + 1] = temp_offsets[row+1];

        //reset other indices
        #pragma unroll
        for(int offset=1; offset<BINS; offset++)
        {
        	A_o[offset*pitch + row*2] = -1;
        	A_o[offset*pitch + row*2 + 1] = -1;
    	}        
    }

    if(tID == 0)
    {
    	Matrix_MD[0] = temp_offsets[num_rows];

    	#pragma unroll
    	for(int offset=1; offset < BINS; offset++)
    	{
			A_o[(num_rows-1)*2 + offset*2] = -1;		//row (num_rows - 1)
			A_o[(num_rows-1)*2 + offset*2 + 1] = -1;
		}
    }
}

template <typename INDEX_TYPE, typename VALUE_TYPE, unsigned int BINS>
__launch_bounds__(BLOCK_THREAD_SIZE,1)
__global__ void
SetRowData(		const INDEX_TYPE num_rows,
				const INDEX_TYPE pitch,
				INDEX_TYPE *Matrix_MD,
				INDEX_TYPE *A_ro,
				INDEX_TYPE *A_rs,
				const INDEX_TYPE *temp_offsets)
{
	const int tID = blockDim.x * blockIdx.x + threadIdx.x;			// global thread index
	const int grid_size = blockDim.x * gridDim.x;
    
    for(INDEX_TYPE row = tID; row < num_rows; row += grid_size)
    {   
        A_ro[row*2] = temp_offsets[row];
        A_ro[row*2 + 1] = temp_offsets[row+1];
        A_rs[row] = temp_offsets[row+1] - temp_offsets[row];

        //reset other indices
        #pragma unroll
        for(int offset=1; offset<BINS; offset++)
        {
        	A_ro[offset*pitch + row*2] = -1;
        	A_ro[offset*pitch + row*2 + 1] = -1;
    	}        
    }

    if(tID == 0)
    {
    	Matrix_MD[0] = temp_offsets[num_rows];

    	#pragma unroll
    	for(int offset=1; offset < BINS; offset++)
    	{
			A_ro[(num_rows-1)*2 + offset*2] = -1;		//row (num_rows - 1)
			A_ro[(num_rows-1)*2 + offset*2 + 1] = -1;
		}
    }
}

//*******************************************************************************//
//Initialize dcsr or dcsr_B matrix
//*******************************************************************************//
template <typename INDEX_TYPE, typename VALUE_TYPE>
__launch_bounds__(BLOCK_THREAD_SIZE,1)
__global__ void 
InitializeMatrix_dcsr(	const INDEX_TYPE num_rows,
						const INDEX_TYPE chunk_length,
						INDEX_TYPE *Matrix_MD,
						INDEX_TYPE *row_offsets)
{
	const int tID = blockDim.x * blockIdx.x + threadIdx.x;
	const int grid_size = blockDim.x * gridDim.x;

	for(INDEX_TYPE row=tID; row < num_rows; row += grid_size)
    {
    	row_offsets[row*2] = row * chunk_length;
    	row_offsets[row*2+1] = row * chunk_length + chunk_length;
    }

	if(tID == 0)
	{
		Matrix_MD[0] = num_rows*chunk_length;
	}
}

}	//namespace device

#endif