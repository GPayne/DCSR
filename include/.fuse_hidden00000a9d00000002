#ifndef SPARSE_UPDATE_DEVICE
#define SPARSE_UPDATE_DEVICE

#define QUEUE_SIZE		512
#define WARP_SIZE 		32
#define LOG_WARP_SIZE	5

namespace device
{

__constant__ int chunk_sizes[8];

template <typename INDEX_TYPE, typename VALUE_TYPE>
__launch_bounds__(BLOCK_THREADS_MAX,1)
__global__ void 
OuterProduct(	const VALUE_TYPE *a,
				const VALUE_TYPE *b,
				const INDEX_TYPE num_rows,
				const INDEX_TYPE num_cols,
				const INDEX_TYPE num_cols_per_row,
				const INDEX_TYPE pitch,
				INDEX_TYPE *column_indices,
				VALUE_TYPE *values)
{
	const int tID = blockDim.x * blockIdx.x + threadIdx.x;
	const int grid_size = blockDim.x * gridDim.x;
	
	const INDEX_TYPE invalid_index = cusp::ell_matrix<int, INDEX_TYPE, cusp::device_memory>::invalid_index;

	__shared__ int entries_b[BLOCK_THREADS];
	__shared__ int num_entries_a, num_entries_b;

	entries_b[threadIdx.x] = -1;
	__syncthreads();

	if(threadIdx.x == 0)		//first thread of every block
	{
		num_entries_a = 0;
		num_entries_b = 0;

		for(int i=0; i < num_rows; ++i)
		{
			if(a[i] != 0)
				num_entries_a++;
		}

		for(int i=0; i < num_cols; ++i)
		{
			if(b[i] != 0)
			{
				entries_b[num_entries_b] = i;
				num_entries_b++;
			}
		}
	}
	__syncthreads();

	for(int row=tID; row<num_rows; row+=grid_size)
	{
		int offset = row;
		if(a[row])
		{
			for(int n=0; n < num_entries_b; ++n, offset+=pitch)
			{
				column_indices[offset] = entries_b[n];
				values[offset] = 1;
			}
		}

		while(offset < num_cols_per_row*pitch)
		{
			column_indices[offset] = invalid_index;
			offset += pitch;
		}
	}
}

//size of a must be num_rows + 1
//size of b must be num_cols + 1
//last entry of each array is used for storing entry count
//only add unique entries
template <typename INDEX_TYPE, typename VALUE_TYPE>
__launch_bounds__(BLOCK_THREADS_MAX,1)
__global__ void 
OuterProductAdd_ELL_B(	const VALUE_TYPE *a,
						const VALUE_TYPE *b,
						const INDEX_TYPE *index_count,
						const INDEX_TYPE num_rows,
						const INDEX_TYPE num_cols,
						const INDEX_TYPE num_cols_per_row,
						const INDEX_TYPE pitch,
						INDEX_TYPE *column_indices,
						VALUE_TYPE *values)
{
	const int tID = threadIdx.x & (WARP_SIZE-1); 									//thread ID
	const int wID = (blockDim.x * blockIdx.x + threadIdx.x) / WARP_SIZE;			//warp ID
	const int grid_size = (blockDim.x * gridDim.x) / WARP_SIZE;
	const INDEX_TYPE invalid_index = cusp::ell_matrix<INDEX_TYPE, VALUE_TYPE, cusp::device_memory>::invalid_index;

	INDEX_TYPE num_entries_a = index_count[0];
	INDEX_TYPE num_entries_b = index_count[1];
	__shared__ INDEX_TYPE row_index[BLOCK_THREADS/WARP_SIZE];

	for(INDEX_TYPE j=wID; j < num_entries_a; j+=grid_size)
	{
		INDEX_TYPE row = a[j];
		row_index[wID] = column_indices[row];
		for(INDEX_TYPE k=tID; k < num_entries_b; k+=WARP_SIZE)
		{
			VALUE_TYPE b_col = b[k];
			INDEX_TYPE offset = row;
			for(INDEX_TYPE n=1; n < num_cols_per_row; ++n, offset+=pitch)
			{
				INDEX_TYPE col = column_indices[offset];
				if(col == b_col)
				{
					break;
				}
				else if(col == invalid_index)
				{
					column_indices[row*(atomicAdd(&row_index[wID],1)+1)] = b_col;
					//values[offset] = 1;
					break;
				}
			}
		}
		if(tID == 0)
			column_indices[row] = row_index[wID];
	}
}

//size of a must be num_rows + 1
//size of b must be num_cols + 1
//last entry of each array is used for storing entry count
//only add unique entries
template <typename INDEX_TYPE, typename VALUE_TYPE>
__launch_bounds__(BLOCK_THREADS_MAX,1)
__global__ void 
OuterProductAdd_HYB_B(	const VALUE_TYPE *a,
						const VALUE_TYPE *b,
						const INDEX_TYPE *index_count,
						const INDEX_TYPE num_rows,
						const INDEX_TYPE num_cols,
						const INDEX_TYPE num_cols_per_row,
						const INDEX_TYPE pitch,
						INDEX_TYPE *row_sizes,
						INDEX_TYPE *column_indices,
						INDEX_TYPE *coo_row_indices,
						INDEX_TYPE *coo_column_indices)
{
	//const int tID = threadIdx.x & (WARP_SIZE-1); 									//thread ID
	//const int wID = (blockDim.x * blockIdx.x + threadIdx.x) / WARP_SIZE;			//warp ID
	//const int grid_size = (blockDim.x * gridDim.x) / WARP_SIZE;
	const int tID = blockDim.x * blockIdx.x + threadIdx.x;
	const int grid_size = blockDim.x * gridDim.x;
	const INDEX_TYPE invalid_index = cusp::ell_matrix<INDEX_TYPE, VALUE_TYPE, cusp::device_memory>::invalid_index;

	INDEX_TYPE num_entries_a = index_count[0];
	INDEX_TYPE num_entries_b = index_count[1];

	//for(INDEX_TYPE j=wID; j < num_entries_a; j+=grid_size)
	for(INDEX_TYPE j=tID; j < num_entries_a; j+=grid_size)
	{
		INDEX_TYPE row = a[j];
		//for(INDEX_TYPE k=tID; k < num_entries_b; k+=WARP_SIZE)
		for(INDEX_TYPE k=0; k<num_entries_b; ++k)
		{
			VALUE_TYPE b_col = b[k];
			INDEX_TYPE offset = row;
			bool overflow = false;
			for(INDEX_TYPE n=1; n < num_cols_per_row; ++n, offset+=pitch)
			{
				INDEX_TYPE col = column_indices[offset];
				if(col == b_col)
				{
					if(n == num_cols_per_row-1)
						overflow = true;
					break;
				}
				else if(col == invalid_index)
				{
					INDEX_TYPE index = atomicAdd(&row_sizes[row], 1);
					column_indices[row + pitch*index] = b_col;
					break;
				}
			}

			//coordinate overflow
			if(overflow)
			{
				bool valid = true;
				for(int i=1; i < coo_column_indices[0]; ++i)
				{
					if(coo_column_indices[i] == b_col && coo_row_indices[i] == row)
					{
						valid = false;
						break;
					}
				}

				if(valid)
				{
					int index = atomicAdd(&coo_column_indices[0], 1)+1;
					coo_row_indices[index] = row;
					coo_column_indices[index] = b_col;
				}
			}
		}
	}
}

//size of a must be num_rows + 1
//size of b must be num_cols + 1
//last entry of each array is used for storing entry count
//only add unique entries
template <typename INDEX_TYPE, typename VALUE_TYPE>
__launch_bounds__(BLOCK_THREADS_MAX,1)
__global__ void 
OuterProductAdd_DELL_B(	const VALUE_TYPE *a,
						const VALUE_TYPE *b,
						const INDEX_TYPE *index_count,
						const INDEX_TYPE num_rows,
						const INDEX_TYPE chunks,
						const INDEX_TYPE chunk_size,
						const INDEX_TYPE groups,
						INDEX_TYPE *ci,
						INDEX_TYPE *cl,
						INDEX_TYPE *cols,
						INDEX_TYPE *overflow_col,
						INDEX_TYPE *overflow_row)
{
	// const int tID = blockDim.x * blockIdx.x + threadIdx.x;
	// const int grid_size = blockDim.x * gridDim.x;
}

//size of a must be num_rows + 1
//size of b must be num_cols + 1
//last entry of each array is used for storing entry count
//only add unique entries
template <typename INDEX_TYPE, typename VALUE_TYPE>
__launch_bounds__(BLOCK_THREADS_MAX,1)
__global__ void 
OuterProductAdd_Queue(	const VALUE_TYPE *a,
						const VALUE_TYPE *b,
						const INDEX_TYPE *index_count,
						INDEX_TYPE *queue)
{
	//const int tID = threadIdx.x & (WARP_SIZE-1); 									//thread ID
	//const int wID = (blockDim.x * blockIdx.x + threadIdx.x) / WARP_SIZE;			//warp ID
	//const int grid_size = (blockDim.x * gridDim.x) / WARP_SIZE;
	const int tID = blockDim.x * blockIdx.x + threadIdx.x;
	const int grid_size = blockDim.x * gridDim.x;
	//const INDEX_TYPE invalid_index = -1;

	INDEX_TYPE num_entries_a = index_count[0];
	INDEX_TYPE num_entries_b = index_count[1];
	//INDEX_TYPE qstart = queue[0];
	INDEX_TYPE qstop = queue[1];

	if(tID == 0)
	{
		queue[qstop] = num_entries_a;
		queue[qstop+1] = num_entries_b;
	}
	__syncthreads();

	for(INDEX_TYPE j=tID; j < num_entries_a; j+=grid_size)
	{
		queue[qstop+2 + j] = a[j];
	}
	
	for(INDEX_TYPE k=tID; k<num_entries_b; k+=grid_size)
	{
		queue[qstop+2 + num_entries_a + k] = b[k];
	}
	__syncthreads();

	if(tID == 0)
	{
		queue[1] += (num_entries_a + num_entries_b + 2);
	}
	__syncthreads();
}

//*****************************************************************************//
//update matrices with arrays of row, column and value indices
//*****************************************************************************//
// template <typename INDEX_TYPE, typename VALUE_TYPE>
// __launch_bounds__(BLOCK_THREADS,1)
// __global__ void 
// UpdateMatrix_dcsr_B(const INDEX_TYPE num_rows,
// 					const INDEX_TYPE chunk_size,
// 					const INDEX_TYPE pitch,
// 					const float alpha,
// 					const INDEX_TYPE *src_rows,
// 					const INDEX_TYPE *src_cols,
// 					const INDEX_TYPE N,
// 					INDEX_TYPE *Matrix_MD,
// 					INDEX_TYPE *ci,
// 					INDEX_TYPE *cl,
// 					INDEX_TYPE *ca,
// 					INDEX_TYPE *rs,
// 					INDEX_TYPE *cols)
// {
// 	const int tID = blockDim.x * blockIdx.x + threadIdx.x;
// 	//const int lID = threadIdx.x;
// 	const int grid_size = blockDim.x * gridDim.x;

// 	//__shared__ volatile INDEX_TYPE rs_s[BLOCK_THREADS];

// 	for(INDEX_TYPE row=tID; row < num_rows; row+=grid_size)
// 	{
// 		//rs_s[lID] = rs[row];
// 		//__syncthreads();

// 		for(INDEX_TYPE i=0; i < N; i++)
// 		{
// 			INDEX_TYPE cID = row / chunk_size;
// 			if(src_rows[i] == row)
// 			{
// 				INDEX_TYPE col = src_cols[i];
// 				//INDEX_TYPE rl = rs_s[lID];
// 				INDEX_TYPE rl = rs[row];
// 				INDEX_TYPE r_idx = 0, c_idx, offset;

// 				bool valid = true;
// 				bool next_chunk = false;

// 				do
// 				{
// 					INDEX_TYPE next_cID = ci[cID];
// 					offset = ca[cID] + (row % chunk_size);
// 					c_idx = 0;

// 					for(; c_idx < cl[cID] && r_idx < rl && valid; c_idx++, r_idx++)
// 					{
// 						//if(cols[offset + c_idx] == col)
// 						if(cols[offset + c_idx*pitch] == col)
// 						{
// 							valid = false;
// 							break;
// 						}
// 					}

// 					if(next_cID > 0 && r_idx < rl)
// 					{
// 						next_chunk = true;
// 						cID = next_cID;
// 					}
// 					else
// 						next_chunk = false;

// 				} while(next_chunk && valid);

// 				if(c_idx < cl[cID] && rl == r_idx && valid)
// 				{
// 					cols[offset + c_idx*pitch] = col;
// 					rs[row] += 1;
// 					//rs_s[lID] += 1;
// 					valid = false;
// 				}
// 				else if(valid)
// 				{
// 					if(atomicCAS(&ci[cID], 0, -1) == 0)
// 					{
// 						INDEX_TYPE chunk_length = alpha*cl[cID];
// 						INDEX_TYPE new_add = atomicAdd(&Matrix_MD[1], chunk_size*chunk_length);
// 						INDEX_TYPE new_cID = atomicAdd(&Matrix_MD[0], 1);

// 						//allocate new block...
// 						cl[new_cID] = chunk_length;
// 						ca[new_cID] = new_add;
// 						ci[cID] = new_cID;
// 					}
					
// 					while(ci[cID] <= 0) {}

// 					if(ci[cID] > 0)
// 					{
// 						cID = ci[cID];
// 						INDEX_TYPE offset = ca[cID] + (row % chunk_size);

// 						if(rl == r_idx)
// 						{
// 							//cuPrintf("inserting col: %d  in row: %d\n", col, row);
// 							cols[offset] = col;
// 							rs[row] += 1;
// 							//rs_s[lID] += 1;
// 						}
// 					}
// 				}
// 			}
// 		}

// 		//rs[row] = rs_s[lID];
// 	}
// }

template <typename INDEX_TYPE, typename VALUE_TYPE>
__launch_bounds__(BLOCK_THREADS,1)
__global__ void 
UpdateMatrix_dcsr(	const INDEX_TYPE num_rows,
					const INDEX_TYPE pitch,
					const INDEX_TYPE *src_rows,
					const INDEX_TYPE *src_cols,
					const VALUE_TYPE *src_vals,
					const INDEX_TYPE N,
					INDEX_TYPE *Matrix_MD,
					INDEX_TYPE *Aj,
					VALUE_TYPE *Ax,
					INDEX_TYPE *row_offsets,
					INDEX_TYPE *row_sizes)
{
	const INDEX_TYPE tID = blockDim.x*blockIdx.x + threadIdx.x;
	const INDEX_TYPE grid_size = blockDim.x * gridDim.x;				//grid size

	for(INDEX_TYPE row=tID; row < num_rows; row+=grid_size)
	{
		//loop of N inputs looking for elements from the respective row
		for(INDEX_TYPE i=0; i < N; i++)
		{
			if(src_rows[i] == row)
			{
				INDEX_TYPE col = src_cols[i];
				VALUE_TYPE val = src_vals[i];
				INDEX_TYPE rl = row_sizes[row];

				bool valid = true;
				INDEX_TYPE offset = 0;
				INDEX_TYPE r_idx = 0, idx, row_start, row_end;
				idx = row_start = row_offsets[row*16];
				row_end = row_offsets[row*16 + 1];

				while(r_idx < rl && valid)
				{
					for(idx = row_start; idx < row_end && r_idx < rl; idx++, r_idx++)
					{
						if(Aj[idx] == col)
						{
							valid = false;
							break;
						}
					}

					if(idx >= row_end)
					{
						offset++;
						idx = row_start = row_offsets[offset*2 + row*16];
						row_end = row_offsets[offset*2 + row*16+1];
					}
				}

				if(valid)
				{
					//if non allocated memory section then allocate new section of memory for this row
					if(row_start == row_end)
					{
						//allocate new space for chunk
						INDEX_TYPE new_size = max((row_offsets[(offset-1)*2 + row*16 + 1] - row_offsets[(offset-1)*2 + row*16]) * (offset+1), 32);
						INDEX_TYPE new_add = atomicAdd(&Matrix_MD[0], new_size);	//increase global memory pointer

						//allocate new row chunk...
						idx = row_offsets[offset*2 + row*16] = new_add;
						row_offsets[offset*2 + row*16+1] = new_add + new_size;
					}

					Aj[idx] = col;
					Ax[idx] = val;
					row_sizes[row] += 1;
				}
			}
		}
	}
}

template <typename INDEX_TYPE, typename VALUE_TYPE>
__launch_bounds__(BLOCK_THREADS,1)
__global__ void 
UpdateMatrix_hyb_B(	const INDEX_TYPE num_rows,
					const INDEX_TYPE num_cols,
					const INDEX_TYPE num_cols_per_row,
					const INDEX_TYPE pitch,
					const INDEX_TYPE *src_rows,
					const INDEX_TYPE *src_cols,
					const INDEX_TYPE N,
					INDEX_TYPE *rs,
					INDEX_TYPE *column_indices,
					INDEX_TYPE *overflow_rows,
					INDEX_TYPE *overflow_cols)
{
	const int tID = blockDim.x * blockIdx.x + threadIdx.x;
	const int grid_size = blockDim.x * gridDim.x;

	for(INDEX_TYPE row=tID; row < num_rows; row+=grid_size)
	{
		for(INDEX_TYPE i=0; i < N; i++)
		{
			if(src_rows[i] == row)
			{
				INDEX_TYPE offset = row;
				INDEX_TYPE col = src_cols[i];
				INDEX_TYPE rl = rs[row];
				bool valid = true;

				for(INDEX_TYPE j=0; j < rl && valid; j++)
				{
					if(column_indices[offset + j*pitch] == col)
					{
						valid = false;
						break;
					}
				}

				if(rl < num_cols_per_row && valid)
				{
					column_indices[offset + rl*pitch] = col;
					rs[row] += 1;
					valid = false;
				}
				else if(valid) 	//overflow
				{
					bool ovf_valid = true;
					for(INDEX_TYPE i=1; i <= rs[num_rows]; ++i)
					{
						if(overflow_cols[i] == col && overflow_rows[i] == row)
						{
							ovf_valid = false;
							break;
						}
					}

					if(ovf_valid)
					{
						INDEX_TYPE index = atomicAdd(&rs[num_rows], 1);
						overflow_rows[index] = row;
						overflow_cols[index] = col;
					}
				}
			}
		}
	}
}

template <typename INDEX_TYPE, typename VALUE_TYPE>
__launch_bounds__(BLOCK_THREADS,1)
__global__ void 
UpdateMatrix_hyb(	const INDEX_TYPE num_rows,
					const INDEX_TYPE num_cols,
					const INDEX_TYPE num_cols_per_row,
					const INDEX_TYPE pitch,
					const INDEX_TYPE *src_rows,
					const INDEX_TYPE *src_cols,
					const VALUE_TYPE *src_vals,
					const INDEX_TYPE idx_offset,
					const INDEX_TYPE N,
					INDEX_TYPE *row_sizes,
					INDEX_TYPE *column_indices,
					VALUE_TYPE *vals,
					INDEX_TYPE *overflow_rows,
					INDEX_TYPE *overflow_cols,
					VALUE_TYPE *overflow_vals)
{
	const int tID = blockDim.x * blockIdx.x + threadIdx.x;
	const int grid_size = blockDim.x * gridDim.x;

	for(INDEX_TYPE row=tID; row < num_rows; row+=grid_size)
	{
		for(INDEX_TYPE i=idx_offset; i < N; i++)
		{
			if(src_rows[i] == row)
			{
				INDEX_TYPE offset = row;
				INDEX_TYPE col = src_cols[i];
				VALUE_TYPE val = src_vals[i];
				INDEX_TYPE rl = row_sizes[row];
				INDEX_TYPE idx;
				bool valid = true;

				for(idx = 0; idx < rl && idx < num_cols_per_row && valid; idx++)
				{
					if(column_indices[offset + idx*pitch] == col)
					{
						valid = false;
						break;
					}
				}

				if(rl < num_cols_per_row && valid)
				{
					column_indices[offset + idx*pitch] = col;
					vals[offset + idx*pitch] = val;
					row_sizes[row] += 1;
					valid = false;
				}
				else if(valid) 	//overflow
				{
					//get end index of sorted values
					INDEX_TYPE send = row_sizes[num_rows+1];
					INDEX_TYPE sstart = 0, mid, pos = -1;

					//binary search of sorted section
					while(sstart != send)
					{
						if(send - sstart <= 1)
						{
							if(overflow_rows[sstart] == row)
								pos = sstart;
							break;
						}

						mid = (send - sstart) / 2 + sstart;
						if(overflow_rows[mid] > row)
							send = mid;
						else if(overflow_rows[mid] < row)
							sstart = mid;
						else if(overflow_rows[mid] == row)
						{	
							for(pos=mid; pos > 0; pos--)
							{
								if(overflow_rows[pos-1] != row)
									break;
							}
							break;
						}
					}

					bool ovf_valid = true;
					if(pos > -1)
					{
						send = row_sizes[num_rows+1];
						for(INDEX_TYPE j=pos; j < send && overflow_rows[row] == row; ++j)
						{
							if(overflow_cols[j] == col)
							{
								ovf_valid = false;
								break;
							}
						}
					}

					sstart = row_sizes[num_rows+1];
					send = row_sizes[num_rows];
					for(INDEX_TYPE j=sstart; j <= send && ovf_valid; ++j)
					{
						if(overflow_cols[j] == col && overflow_rows[j] == row)
						{
							ovf_valid = false;
							break;
						}
					}

					if(ovf_valid)
					{
						INDEX_TYPE index = atomicAdd(&row_sizes[num_rows], 1);
						row_sizes[row] += 1;
						overflow_rows[index] = row;
						overflow_cols[index] = col;
						overflow_vals[index] = val;
					}
				}
			}
		}
	}
}

//*******************************************************************************//
//Load matrix from coo matrix.  Assume no duplicate entries.
//*******************************************************************************//
template <typename INDEX_TYPE, typename VALUE_TYPE>
__launch_bounds__(BLOCK_THREADS,1)
__global__ void 
LoadMatrix_dcsr_B_coo(	const INDEX_TYPE num_rows,
						const INDEX_TYPE chunk_size,
						const INDEX_TYPE pitch,
						const float alpha,
						const INDEX_TYPE *src_rows,
						const INDEX_TYPE *src_cols,
						const INDEX_TYPE N,
						INDEX_TYPE *Matrix_MD,
						INDEX_TYPE *ci,
						INDEX_TYPE *cl,
						INDEX_TYPE *ca,
						INDEX_TYPE *rs,
						INDEX_TYPE *cols)
{
	const int tID = blockDim.x * blockIdx.x + threadIdx.x;		//thread ID
	//const int lID = threadIdx.x;								//lane ID
	const int grid_size = blockDim.x * gridDim.x;				//grid size

	for(INDEX_TYPE row=tID; row < num_rows; row+=grid_size)
	{

	}
}

template <typename INDEX_TYPE, typename VALUE_TYPE>
__launch_bounds__(BLOCK_THREADS,1)
__global__ void 
LoadMatrix_dcsr_coo(	const INDEX_TYPE num_rows,
						const INDEX_TYPE chunk_size,
						const INDEX_TYPE pitch,
						const float alpha,
						const INDEX_TYPE *src_rows,
						const INDEX_TYPE *src_cols,
						const VALUE_TYPE *src_vals,
						const INDEX_TYPE N,
						INDEX_TYPE *Matrix_MD,
						INDEX_TYPE *ci,
						INDEX_TYPE *cl,
						INDEX_TYPE *ca,
						INDEX_TYPE *rs,
						INDEX_TYPE *cols,
						VALUE_TYPE *vals)
{
	// const int tID = blockDim.x * blockIdx.x + threadIdx.x;		//thread ID
	// const int btID = threadIdx.x;								//block thread ID
	// const int grid_size = blockDim.x * gridDim.x;				//grid size

	// __shared__ volatile INDEX_TYPE rs_s[BLOCK_THREADS];

	// for(INDEX_TYPE row=tID; row < num_rows; row+=grid_size)
	// {

	// }
}

template <typename INDEX_TYPE, typename VALUE_TYPE>
__launch_bounds__(BLOCK_THREADS,1)
__global__ void 
LoadMatrix_hyb_B_coo(	const INDEX_TYPE num_rows,
						const INDEX_TYPE num_cols,
						const INDEX_TYPE num_cols_per_row,
						const INDEX_TYPE pitch,
						const INDEX_TYPE *src_rows,
						const INDEX_TYPE *src_cols,
						const INDEX_TYPE N,
						INDEX_TYPE *rs,
						INDEX_TYPE *column_indices,
						VALUE_TYPE *vals,
						INDEX_TYPE *overflow_rows,
						INDEX_TYPE *overflow_cols)
{
	const int tID = blockDim.x * blockIdx.x + threadIdx.x;
	const int grid_size = blockDim.x * gridDim.x;

	for(INDEX_TYPE row=tID; row < num_rows; row+=grid_size)
	{
		for(INDEX_TYPE i=0; i < N; i++)
		{
			if(src_rows[i] == row)
			{
				INDEX_TYPE offset = row;
				INDEX_TYPE col = src_cols[i];
				INDEX_TYPE rl = rs[row];

				if(rl < num_cols_per_row)
				{
					column_indices[offset + rl*pitch] = col;
					rs[row] += 1;
				}
				else //overflow
				{
					INDEX_TYPE index = atomicAdd(&rs[num_rows], 1);
					rs[row] += 1;
					overflow_rows[index] = row;
					overflow_cols[index] = col;
				}
			}
		}
	}
}

template <typename INDEX_TYPE, typename VALUE_TYPE>
__launch_bounds__(BLOCK_THREADS,1)
__global__ void 
LoadMatrix_hyb_coo(	const INDEX_TYPE num_rows,
					const INDEX_TYPE num_cols,
					const INDEX_TYPE num_cols_per_row,
					const INDEX_TYPE pitch,
					const INDEX_TYPE *src_rows,
					const INDEX_TYPE *src_cols,
					const VALUE_TYPE *src_vals,
					const INDEX_TYPE N,
					INDEX_TYPE *rs,
					INDEX_TYPE *column_indices,
					VALUE_TYPE *vals,
					INDEX_TYPE *overflow_rows,
					INDEX_TYPE *overflow_cols,
					VALUE_TYPE *overflow_vals)
{
	const int tID = blockDim.x * blockIdx.x + threadIdx.x;
	const int grid_size = blockDim.x * gridDim.x;

	for(INDEX_TYPE row=tID; row < num_rows; row+=grid_size)
	{
		INDEX_TYPE offset = row;
		INDEX_TYPE rl = 0;

		for(INDEX_TYPE i=0; i < N; i++)
		{
			if(src_rows[i] == row)
			{
				INDEX_TYPE col = src_cols[i];
				VALUE_TYPE val = src_vals[i];
				
				if(rl < num_cols_per_row)
				{
					column_indices[offset + rl*pitch] = col;
					vals[offset + rl*pitch] = val;
					rl++;
				}
				else //overflow
				{
					INDEX_TYPE index = atomicAdd(&rs[num_rows], 1);
					overflow_rows[index] = row;
					overflow_cols[index] = col;
					overflow_vals[index] = val;
					rl++;
				}
			}
		}

		rs[row] = rl;
	}
}

template <typename INDEX_TYPE, typename VALUE_TYPE>
__launch_bounds__(BLOCK_THREADS,1)
__global__ void 
LoadMatrix_csr_coo(	const INDEX_TYPE num_rows,
					const INDEX_TYPE *src_rows,
					const INDEX_TYPE *src_cols,
					const VALUE_TYPE *src_vals,
					const INDEX_TYPE N,
					INDEX_TYPE *T_i,
					INDEX_TYPE *A_j,
					VALUE_TYPE *A_x)
{
	const int tID = blockDim.x * blockIdx.x + threadIdx.x;
	const int grid_size = blockDim.x * gridDim.x;

	for(INDEX_TYPE i=tID; i < N; i+=grid_size)
	{
		T_i[i] = src_rows[i];
		A_j[i] = src_cols[i];
		A_x[i] = src_vals[i];
	}
}

template <typename INDEX_TYPE, typename VALUE_TYPE>
__launch_bounds__(BLOCK_THREADS,1)
__global__ void 
LoadMatrix_csr_count_rows(	const INDEX_TYPE num_rows,
							const INDEX_TYPE *src_rows,
							const INDEX_TYPE N,
							INDEX_TYPE *row_offsets)
{
	const int tID = blockDim.x * blockIdx.x + threadIdx.x;
	const int grid_size = blockDim.x * gridDim.x;

	for(INDEX_TYPE row=tID; row < num_rows; row+=grid_size)
	{
		for(INDEX_TYPE i=0; i < N; i++)
		{
			if(src_rows[i] == row)
			{
				row_offsets[row] += 1;
			}
		}
	}
}

template <typename INDEX_TYPE, typename VALUE_TYPE>
__launch_bounds__(BLOCK_THREADS,1)
__global__ void 
ConvertMatrix_DELL_CSR(	const INDEX_TYPE num_rows,
						const INDEX_TYPE chunk_size,
						const INDEX_TYPE pitch,
						const INDEX_TYPE *Matrix_MD,
						const INDEX_TYPE *ci,
						const INDEX_TYPE *cl,
						const INDEX_TYPE *ca,
						const INDEX_TYPE *rs,
						const INDEX_TYPE *src_column_indices,
						const VALUE_TYPE *src_values,
						INDEX_TYPE *row_offsets,
						INDEX_TYPE *dst_column_indices,
						VALUE_TYPE *dst_values)
{
	const int tID = blockDim.x * blockIdx.x + threadIdx.x;
	const int grid_size = blockDim.x * gridDim.x;

	for(INDEX_TYPE row=tID; row < num_rows; row += grid_size)
    {
        INDEX_TYPE rl = rs[row];
        INDEX_TYPE r_idx = 0;
        INDEX_TYPE cID = row / chunk_size;
        bool next_chunk = false;

        do
        {
            INDEX_TYPE next_cID = ci[cID];
            //INDEX_TYPE offset = A_ca[cID] + (row % chunk_size);
            INDEX_TYPE offset = ca[cID] + (row & (chunk_size-1))*pitch;
            INDEX_TYPE clength = cl[cID];

            INDEX_TYPE csr_row_start = row_offsets[row];
            //csr_row_end = row_offsets[row+1];

            for(INDEX_TYPE c_idx = 0; c_idx < clength && r_idx < rl; ++c_idx, ++r_idx)
            {
            	dst_column_indices[csr_row_start + r_idx] = src_column_indices[offset + c_idx];
                dst_values[csr_row_start + r_idx] = src_values[offset + c_idx];
            }

            if(next_cID > 0 && r_idx < rl)
            {
                next_chunk = true;
                cID = next_cID;
            }
            else
                next_chunk = false;

        } while(next_chunk);
    }
}

//*******************************************************************************//
//Sort DELL matrix rows
//*******************************************************************************//
template <typename INDEX_TYPE, typename VALUE_TYPE, unsigned int VECTORS_PER_BLOCK, unsigned int THREADS_PER_VECTOR>
__launch_bounds__(BLOCK_THREADS,1)
__global__ void
SetRowIndices(	const INDEX_TYPE num_rows,
				INDEX_TYPE *Ai,
				const INDEX_TYPE *A_row_offsets,
				const INDEX_TYPE *A_rs)
{
    __shared__ volatile INDEX_TYPE ptrs[VECTORS_PER_BLOCK][2];

    const INDEX_TYPE THREADS_PER_BLOCK = VECTORS_PER_BLOCK * THREADS_PER_VECTOR;

    const INDEX_TYPE thread_id   = THREADS_PER_BLOCK * blockIdx.x + threadIdx.x;    // global thread index
    const INDEX_TYPE thread_lane = threadIdx.x & (THREADS_PER_VECTOR - 1);          // thread index within the vector
    const INDEX_TYPE vector_id   = thread_id   /  THREADS_PER_VECTOR;               // global vector index
    const INDEX_TYPE vector_lane = threadIdx.x /  THREADS_PER_VECTOR;               // vector index within the block
    const INDEX_TYPE num_vectors = VECTORS_PER_BLOCK * gridDim.x;                   // total number of active vectors

    for(INDEX_TYPE row = vector_id; row < num_rows; row += num_vectors)
    {
    	INDEX_TYPE r_idx = 0;
        const INDEX_TYPE rl = A_rs[row];

    	#pragma unroll
        for(INDEX_TYPE offset = 0; offset < MAX_OFFSET; offset++)
        {
            // use two threads to fetch A_row_offsets[row] and A_row_offsets[row+1]
            // this is considerably faster than the straightforward version
            if(thread_lane < 2)
            {
                ptrs[vector_lane][thread_lane] = A_row_offsets[offset*2 + row*16 + thread_lane];
                //ptrs[vector_lane][1] = A_row_offsets[offset*2 + row*16 + 1];
            }

            const INDEX_TYPE row_start = ptrs[vector_lane][0];          	//same as: row_start = A_row_offsets[row];
            INDEX_TYPE row_end = ptrs[vector_lane][1];						//same as: row_end   = A_row_offsets[row+1];
            INDEX_TYPE row_startE = row_end;
            const INDEX_TYPE row_endE = row_end;

            if(row_start == row_end)
            	break;					//exit out of loop if range size is 0

            if(row_end - row_start > rl - r_idx)
            {
            	row_end = row_startE = row_start + rl - r_idx;
            }

            if(THREADS_PER_VECTOR == 32 && row_end - row_start > 32)
            {
                // ensure aligned memory access to Aj and Ax
                INDEX_TYPE jj = row_start - (row_start & (THREADS_PER_VECTOR - 1)) + thread_lane;

                // accumulate local sums
                if(jj >= row_start && jj < row_end)
            		Ai[jj] = row;

                // accumulate local sums
                for(jj += THREADS_PER_VECTOR; jj < row_end; jj += THREADS_PER_VECTOR)
                    Ai[jj] = row;
            }
            else
            {
                // accumulate local sums
                for(INDEX_TYPE jj = row_start + thread_lane; jj < row_end; jj += THREADS_PER_VECTOR)
                    Ai[jj] = row;
            }

            //assign extra row slots with a value of -1
            for(INDEX_TYPE jj = row_startE + thread_lane; jj < row_endE; jj += THREADS_PER_VECTOR)
            	Ai[jj] = INT_MAX;

            r_idx = min(rl, r_idx + (row_end - row_start));
        }
    }
}

template <typename INDEX_TYPE, typename VALUE_TYPE>
__launch_bounds__(BLOCK_THREADS,1)
__global__ void
SetRowOffsets(	const INDEX_TYPE num_rows,
				INDEX_TYPE *Matrix_MD,
				const INDEX_TYPE *Ai,
				INDEX_TYPE *A_ro,
				const INDEX_TYPE *A_rs,
				const INDEX_TYPE *temp_offsets)
{
	const int tID = blockDim.x * blockIdx.x + threadIdx.x;			// global thread index
	const int grid_size = blockDim.x * gridDim.x;
    
    for(INDEX_TYPE row = tID; row < num_rows-1; row += grid_size)
    {   
        A_ro[row*16] = temp_offsets[row];
        A_ro[row*16 + 1] = temp_offsets[row+1];

        //reset other indices
        #pragma unroll
        for(int offset=1; offset<MAX_OFFSET; offset++)
        {
        	A_ro[row*16 + offset*2] = 0;
        	A_ro[row*16 + offset*2 + 1] = 0;
    	}        
    }

    if(tID == 0)
    {
    	A_ro[(num_rows-1)*16] = temp_offsets[num_rows-1];
    	Matrix_MD[0] = A_ro[(num_rows-1)*16 + 1] = temp_offsets[num_rows-1] + A_rs[num_rows-1];

    	#pragma unroll
    	for(int offset=1; offset < MAX_OFFSET; offset++)
    	{
			A_ro[(num_rows-1)*16 + offset*2] = 0;		//row (num_rows - 1)
			A_ro[(num_rows-1)*16 + offset*2 + 1] = 0;
		}
    }
}

//*******************************************************************************//
//Initialize dcsr or dcsr_B matrix
//*******************************************************************************//
template <typename INDEX_TYPE, typename VALUE_TYPE>
__launch_bounds__(BLOCK_THREADS,1)
__global__ void 
InitializeMatrix_dcsr(	const INDEX_TYPE num_rows,
						const INDEX_TYPE chunk_length,
						INDEX_TYPE *Matrix_MD,
						INDEX_TYPE *row_offsets)
{
	const int tID = blockDim.x * blockIdx.x + threadIdx.x;
	const int grid_size = blockDim.x * gridDim.x;

	for(INDEX_TYPE row=tID; row < num_rows; row += grid_size)
    {
    	row_offsets[row*16] = row * chunk_length;
    	row_offsets[row*16+1] = row * chunk_length + chunk_length;
    }

	if(tID == 0)
	{
		Matrix_MD[0] = num_rows*chunk_length;
	}
}

}	//namespace device

#endif